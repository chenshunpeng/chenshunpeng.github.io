---
title: saless.csv数据的导入，清洗，分析，导出，可视化（利用hive，sqoop，mysql工具）
commentable: true
Edit: 2022-07-14
mathjax: true
mermaid: true
tags: hive sqoop
categories: 大数据
description: 利用hive，sqoop，mysql工具进行saless.csv数据的导入，清洗，分析，导出，可视化
---

# 题目描述

样表（sales_sample_20170310）字段说明：
>day_id 日期编号；
sale_nbr 卖出方代码；
buy_nbr 买入方代码；
cnt 数量
round 金额

卖出方和买入方又分为 3 种类型：

 - 以'C'开头的表示类型为 C，代表“航空公司”，只可以卖出，不可以买入；
 - 以'O'开头的表示类型为 O，代表“代理人”，既可以卖出，也可以买入，并且允许自己卖给自己（简单来讲：每个“代理人”代码可能对应多个售票点，售票点之前有交换票的情况，所以体现为自己卖给了自己）；
 - 'PAX'表示类型为 PAX，代表“旅客”，只可以买入，不可以卖出。

举例：

>day_id,sale_nbr,buy_nbr,cnt,round1,C1,O1001,1,360
卖出方为 C1，类型为 C；买入方为 O1001，类型为 O
day_id,sale_nbr,buy_nbr,cnt,round1,O100,O100,4,2000
卖出方为 O100，类型为 O；买入方为 O100，类型为 O（即自己卖给自己是允许的）
day_id,sale_nbr,buy_nbr,cnt,round1,O100,PAX,4,2000
卖出方为 O100，类型为 O；买入方为 PAX，类型为 PAX

问题：

**1、数据导入：**
要求将样表文件中的（sales_sample_20170310）数据导入 HIVE 数据仓库中。

**2、数据清洗：**
要求将 day_id 一列中的数值清洗为真实的日期格式，可用字符串表示。
数据 1 对应日期 2021-09-01，依次类推，15 对应日期 2021-09-15

**3、数据分析处理：**

>
>（1）统计每天各个机场的销售数量和销售金额。
要求的输出字段
day_id,sale_nbr,,cnt,round
日期编号，卖出方代码，数量，金额 
（2）统计每天各个代理商的销售数量和销售金额。
要求的输出字段
day_id,sale_nbr,,cnt,round
日期编号，卖出方代码，数量，金额
（3）统计每天各个代理商的销售活跃度。
要求的输出字段
day_id,sale_nbr, sale_number
日期编号，卖出方代码，交易次数（买入或者卖出均算交易次数）
（4）汇总统计 9 月 1 日到 9 月 15 日之间各个代理商的销售利润。
编号，卖出方代码，买入数量，买入金额，卖出数量，卖出金额，销售利润（卖出金额-买入金额）
（5）设计分析代理商的市场地位根据市场交易次数、交易对象个数、销售机票数量、销售利润等（选做题）
 
**4、处理结果入库：**
将上述统计分析的结果数据保存到 mySQL 数据库中。

**5、数据可视化展示：**
利用 Echarts 将上述统计结果以图形化展示的方式展现出来：饼图、柱状图、地图、折线图等。
# hadoop配置
首先安装好hadoop，我参考的b站视频：

>[Hadoop保姆级超详细安装教程](https://www.bilibili.com/video/BV1Kf4y1z7Nw?p=2&vd_source=0d5e3e82bca1726940873040a5b2bb1d)

登录3台虚拟机（均可联网）：

主机（有桌面，Firefox）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/e096aa3d042a4edfadeb5debaf72f2d2.png)
从机1（无桌面）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/64c322e614ee4887a78554e32b7970a9.png)
从机2（无桌面）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/c8219455d7194805a80f1c69ce7ab3f7.png)

注：打开主机终端方式如下

![在这里插入图片描述](https://img-blog.csdnimg.cn/ea2adbe6fe844c62a1109105d074d6f5.png)

`vi /etc/hosts`查看master的host文件：

![在这里插入图片描述](https://img-blog.csdnimg.cn/c790e406329e4672ba372bf4a1028e10.png)

之后 ping s1，没问题

![在这里插入图片描述](https://img-blog.csdnimg.cn/0d907c6419ae46c2a878c29c346a5d90.png)

可是xshell连接失败：

![在这里插入图片描述](https://img-blog.csdnimg.cn/bc294a374ae645a59bceb1863eee1bf6.png)

记得开启了 sshd 服务，再检查一遍，发现果然已经启动了：

![在这里插入图片描述](https://img-blog.csdnimg.cn/d46d2df501f54fdca7954300f0bf1fd8.png)

之后发现子网ip竟然和上面的不对应，改过来就好了（记得不仅要重启虚拟机，还要重启电脑）

![在这里插入图片描述](https://img-blog.csdnimg.cn/de02f75400a6424c9b43b8a3489519fc.png)

这样就可以了：

![在这里插入图片描述](https://img-blog.csdnimg.cn/a5fba82f650c4dc0b039aab2d3c32e26.png)

输入命令`start-all.sh`启动hadoop：

![在这里插入图片描述](https://img-blog.csdnimg.cn/fc6915cc230e4032b7b790209b3341df.png)

输入`jps`有：

![在这里插入图片描述](https://img-blog.csdnimg.cn/694d0bd27e8942359569c71cda943cf4.png)

通过`hadoop jar ~/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar
`命令看一下里面example里面自带的一些程序：

![在这里插入图片描述](https://img-blog.csdnimg.cn/1f5da4ef53094ca1accca2e92282de00.png)

用`hadoop jar ~/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 10 10`尝试计算圆周率：

![在这里插入图片描述](https://img-blog.csdnimg.cn/85f91deb1d854834b4e2a204b0b3add9.png)

输入 [http://master:8088/cluster](http://master:8088/cluster)，访问集群，可以看到执行的这个任务：

![在这里插入图片描述](https://img-blog.csdnimg.cn/e1790e80d054474ebccd777bdb9169ca.png)

输入 [http://master:50070/explorer.html#/wordcount/output](http://master:50070/explorer.html#/wordcount/output)，可以看到我之前操作的一些文件信息：

![在这里插入图片描述](https://img-blog.csdnimg.cn/18fe0982aee14c1bb75bf205ab762670.png)
# hive配置
我参考的b站视频：

>[【Hive教程】](https://www.bilibili.com/video/BV1864y1Q7Li?vd_source=0d5e3e82bca1726940873040a5b2bb1d)

hive版本要和自己的hadoop版本符合才可以：

[https://hive.apache.org/downloads.html](https://hive.apache.org/downloads.html)

![在这里插入图片描述](https://img-blog.csdnimg.cn/d75abf15e78643028551efc8f1aa1b71.png)

[https://dlcdn.apache.org/hive/hive-2.3.9/](https://dlcdn.apache.org/hive/hive-2.3.9/)


![在这里插入图片描述](https://img-blog.csdnimg.cn/a09c5e21db2240fbabaed9e86fdfd76b.png)
启动hive

![在这里插入图片描述](https://img-blog.csdnimg.cn/4b9673c0c15a4011bdce5e2871ff99b8.png)

`create table test(id int);`建立一个test表

![在这里插入图片描述](https://img-blog.csdnimg.cn/926a49bf161045b5a5b2cecd673b8450.png)

`insert into test values(1);`向test表中插入数据1

![在这里插入图片描述](https://img-blog.csdnimg.cn/b6cadb207a6147c8ae9f066dd61c335f.png)

下载可以看到

![在这里插入图片描述](https://img-blog.csdnimg.cn/dfe900193ba348b293ba55984dcc385b.png)
# mysql配置
我参考的b站视频：

>[尚硅谷大数据Hive教程](https://www.bilibili.com/video/BV1EZ4y1G7iL?vd_source=0d5e3e82bca1726940873040a5b2bb1d)

输入`schematool -initSchema -dbType mysql -verbose`初始化hive元数据库：

![在这里插入图片描述](https://img-blog.csdnimg.cn/9d0c826f435b4575931a55505f616a1a.png)

再次启动hive：

![在这里插入图片描述](https://img-blog.csdnimg.cn/be406ad1615a4951bb9858d833e5b327.png)

`create table test(id int);`建立一个test表；
`insert into test values(1);`向test表中插入数据2

![在这里插入图片描述](https://img-blog.csdnimg.cn/dd2bd3408e7f47369d6959954744cfe5.png)

`select * from test;`查询，神奇的发现1这个数据还在

![在这里插入图片描述](https://img-blog.csdnimg.cn/1ab398368b384ce680ae0048c63e883f.png)

所以我们可以认为，只要往原路径下放数据，它均可以查到，而且建表和放数据的顺序随意

我们看下mysql数据库：

![在这里插入图片描述](https://img-blog.csdnimg.cn/339ad539e164499f8a67c18b855dfa9b.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/782f706f6caa42e0b9a838f615b48858.png)
# 完成问题1，2
 在hive里面建表：
 
具体语法可看：[Hive建表语句详解--CREATE TABLE](https://blog.csdn.net/Thomson617/article/details/86153924)
```sql
create table test0 (
    day_id string,
    sale_nbr  string,
    buy_nbr  string,
    cnt  int,
    round double
) 
ROW format delimited fields terminated by ',' STORED AS TEXTFILE;
```
结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/a4c5e35da555447e87af725039acb90f.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/e1d23364eab148c7b95799d7112503fc.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/56490ee6698d4f269a900f3bf489ea40.png)

`hadoop fs -put /opt/software/sales.csv /user/hive/warehouse/test0`导入sales.csv文件；
`select * from test0 limit 10;`查看前10条记录：

![在这里插入图片描述](https://img-blog.csdnimg.cn/497a146d042b45ee867525668c11e79f.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/ea0dba0cf7a648a181d20ca478af99fc.png)
然后把日期清洗成标准格式：

cast()函数用法可看：[mysql cast( as int) error_Mysql SQL CAST()函数](https://blog.csdn.net/weixin_42297982/article/details/113342554)
```sql
insert overwrite table test0 
    select date_add('2021-09-00',cast(day_id as int)) as day_id,
    sale_nbr as sale_nbr,
    buy_nbr as buy_nbr,
    cnt as cnt,
    round as round 
from test0;
```
结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/1b0a7e6526614e909b8c7d24403b423a.png)

`select * from test0 limit 10;`
查看前10条记录：

![在这里插入图片描述](https://img-blog.csdnimg.cn/b580aba9a6be4772859cea96da7465fe.png)
# zookeeper、hbase配置
借鉴：

[【ZooKeeper】安装教程](https://www.bilibili.com/video/BV1gm4y1S7sP?vd_source=0d5e3e82bca1726940873040a5b2bb1d)
[hbase安装教程（操作简单可复制，含zookeeper安装）](https://www.bilibili.com/video/BV12Y4y1b7x8?vd_source=0d5e3e82bca1726940873040a5b2bb1d)
[zookeeper快速入门一：zookeeper安装与启动](https://blog.csdn.net/lamfang/article/details/108866448?spm=1001.2101.3001.6650.7&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-7-108866448-blog-80933365.pc_relevant_multi_platform_whitelistv1)

`zkServer.sh start`

![在这里插入图片描述](https://img-blog.csdnimg.cn/c6cd84f01dd0492681e5d050869d42e1.png)

`start-hbase.sh`

![在这里插入图片描述](https://img-blog.csdnimg.cn/2b23ab3ea10c4cc1be3751e5df13940f.png)

# sqoop配置
sqoop要和hadoop版本对应，这里下的1.4.7版本

[http://archive.apache.org/dist/sqoop/1.4.7/](http://archive.apache.org/dist/sqoop/1.4.7/)

![在这里插入图片描述](https://img-blog.csdnimg.cn/e6d81d7428904aafb3fbe235c7b8e9ed.png)

将文件放进Linux：

![在这里插入图片描述](https://img-blog.csdnimg.cn/42b5264b369c487c9c7261a58ef271e5.png)
之后借鉴博客：

[Sqoop安装与配置](https://blog.csdn.net/qq_51048727/article/details/112685189?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165769525216781432953417%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165769525216781432953417&biz_id=0&spm=1018.2226.3001.4187)

`vi ~/.bash_profile`配置环境变量，`source ~/.bash_profile`使其立即生效：

![在这里插入图片描述](https://img-blog.csdnimg.cn/f73017ac7515406093aaf144b51ad62b.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/b088ca0820ae4545adbd0ba617846f45.png)

`vi sqoop-env.sh`修改已经安装了的Hadoop组件

![在这里插入图片描述](https://img-blog.csdnimg.cn/1f3474f97c9241a5acb7d5f47728112c.png)
`sqoop-version`看版本号：

![在这里插入图片描述](https://img-blog.csdnimg.cn/16004fef79674a9386221eb521fb6324.png)

发现有一些警告，想要去除（但是我hbase和zookeeper都配了，hbase都识别了，zookeeper却识别不了，吐了）：

借鉴：[解决安装sqoop后出现一些警告提示](https://blog.csdn.net/weixin_43716338/article/details/106752816)

![在这里插入图片描述](https://img-blog.csdnimg.cn/ed47baa338244dce81c6027053415247.png)

之后就可以了：

![在这里插入图片描述](https://img-blog.csdnimg.cn/6263f3d26ecf462c83a66e0bded3e125.png)
# 利用sqoop将hive数据导出到mysql
借鉴博客：

[利用sqoop将数据从hive导入mysql时报错](https://blog.csdn.net/weixin_47158773/article/details/124161038?spm=1001.2101.3001.6650.15&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-15-124161038-blog-96262815.pc_relevant_multi_platform_whitelistv1)

[利用sqoop将hive数据导入导出数据到mysql](http://www.javashuo.com/article/p-xeewcmlu-hq.html)

列出mysql数据库中的全部数据：
`sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password xxx`

![在这里插入图片描述](https://img-blog.csdnimg.cn/4f3a86e1d223462b8d370dcf1eb29978.png)

将hive中的表数据导入到mysql中（表结构就跟你hive一样就行）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/016544e7999543cd9e88b7c6bffd5fbe.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/d07a5b03e2414625a28f3f15a21bf6ed.png)
之后执行sql：
```sql
bin/sqoop export 
--connect jdbc:mysql://master:3306/sales 
--username root 
--password xxx 
--table test0 
--num-mappers 1 
--export-dir /user/hive/warehouse/test0 
--input-fields-terminated-by ","
```
sql语句解释：

```sql
bin/sqoop export    #表示数据从 hive 复制到 mysql 中
–username root      #mysql登陆用户名
–password xxx       #登录密码
–table test0        #mysql中的表，即将被导入的表名称
–export-dir /user/hive/warehouse/test0  #hive中被导出的文件
–input-fields-terminated-by ","         #Hive 中被导出的文件字段的分隔符
```

运行结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/63446636b5684778a626f47e3054ac75.png)

排查原因，发现由于hive表里面没有主键，因此mysql里面不应该有主键，应该把主键去了：

![在这里插入图片描述](https://img-blog.csdnimg.cn/8cc9b66f4b5141ca878a0b42d836d813.png)

之后执行代码：

![在这里插入图片描述](https://img-blog.csdnimg.cn/fa5af8be8ea9481aa019c0d25ece549f.png)

结果如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/04e4a73d3fcc454e971ac68f5ed83bf5.png)
# 注
对于之后的问题，做完会同步到博客上
